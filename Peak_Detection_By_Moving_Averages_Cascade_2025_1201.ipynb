{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqYxw5tEycXd",
        "outputId": "859d4712-f00f-4c57-db49-673f55fbe8c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "511"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "511"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import csv\n",
        "import gc\n",
        "import shutil\n",
        "from collections import deque\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION & SETUP\n",
        "# ==========================================\n",
        "\n",
        "# --- USER CONFIGURATION (Colab Forms) ---\n",
        "# Default is set to 2000 Hz as requested.\n",
        "# If using the 5000 Hz file, change this value to 5000 in the side panel.\n",
        "INPUT_FILENAME = \"resampled_signal_2000.csv\"  # @param {type:\"string\"}\n",
        "SAMPLE_RATE_HZ = 2000  # @param {type:\"integer\"}\n",
        "\n",
        "# --- SYSTEM CONSTANTS ---\n",
        "# Derived calculations based on the Sample Rate\n",
        "SAMPLE_RATE_MS = 1000 / SAMPLE_RATE_HZ\n",
        "TIME_DELTA_SEC = 1.0 / SAMPLE_RATE_HZ\n",
        "\n",
        "print(f\"--- System Configuration ---\")\n",
        "print(f\"Target Input File: {INPUT_FILENAME}\")\n",
        "print(f\"Sample Rate: {SAMPLE_RATE_HZ} Hz (Delta: {TIME_DELTA_SEC:.6f} s)\")\n",
        "\n",
        "# --- DIRECTORY SETUP ---\n",
        "# Creates a local output directory to avoid dependency on specific Drive folders\n",
        "BASE_DIR = os.getcwd()\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, 'output')\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- OUTPUT FILE PATHS ---\n",
        "TIMESTAMP_STR = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "FILE_PATH = os.path.join(BASE_DIR, INPUT_FILENAME)\n",
        "OUT_LOG_FILE = os.path.join(OUTPUT_DIR, f'detection_log_{TIMESTAMP_STR}.csv')\n",
        "OUT_DUMP_FILE = os.path.join(OUTPUT_DIR, f'segments_dump_{TIMESTAMP_STR}.csv')\n",
        "OUT_STATS_FILE = os.path.join(OUTPUT_DIR, f'processing_stats_{TIMESTAMP_STR}.txt')\n",
        "\n",
        "# --- MOVING AVERAGES (MA) ---\n",
        "# Windows defined in milliseconds\n",
        "MA_WINDOWS_MS = [1, 3, 8, 21, 55, 144, 377, 987, 2584, 6765]\n",
        "MA_KEYS = [f'MA{ms:04d}' for ms in MA_WINDOWS_MS]\n",
        "# Calculate sample size for each MA based on the current Hz\n",
        "MA_SAMPLES_MAP = {k: int(ms / SAMPLE_RATE_MS) for k, ms in zip(MA_KEYS, MA_WINDOWS_MS)}\n",
        "\n",
        "# Logic Groups\n",
        "CASCADE_KEYS = ['MA0001', 'MA0003', 'MA0008', 'MA0021', 'MA0055']\n",
        "SLOW_KEYS = ['MA0144', 'MA0377', 'MA0987', 'MA2584', 'MA6765']\n",
        "BASE_MA_KEY = 'MA0987'\n",
        "\n",
        "# --- LOGIC THRESHOLDS ---\n",
        "BUFFER_SIZE_MS = 2000  # 2 seconds circular buffer\n",
        "BUFFER_LEN = int(BUFFER_SIZE_MS / SAMPLE_RATE_MS)\n",
        "\n",
        "PERSISTENCE_MAYOR_MS = 7\n",
        "PERSISTENCE_MAYOR_SAMPLES = int(PERSISTENCE_MAYOR_MS / SAMPLE_RATE_MS)\n",
        "\n",
        "WARMUP_PERIOD_MS = 8000\n",
        "WARMUP_SAMPLES = int(WARMUP_PERIOD_MS / SAMPLE_RATE_MS)\n",
        "\n",
        "# Recording Constraints\n",
        "MAX_PEAK_DURATION_MS = 70.0  # Force exit (occlusion) logic\n",
        "RECORDING_WINDOW_MS = 200.0  # Total duration to save\n",
        "SNIPPET_PRE_MS = 15.0        # Context before event\n",
        "\n",
        "# Plotting Constants\n",
        "PLOT_PRE_START_MS = 15.0\n",
        "PLOT_POST_START_MS = 85.0\n",
        "PLOT_DPI = 200\n",
        "PLOT_FIGSIZE = (20, 12)\n",
        "\n",
        "# --- CHANNEL SPECIFICS ---\n",
        "CONFIG_ADC1 = {\n",
        "    'name': 'ADC1',\n",
        "    'mayor_thresh': 600.0,\n",
        "    'minor_offset': 60.0\n",
        "}\n",
        "\n",
        "CONFIG_ADC2 = {\n",
        "    'name': 'ADC2',\n",
        "    'mayor_thresh': 800.0,\n",
        "    'minor_offset': 80.0\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# 2. DATA STRUCTURES\n",
        "# ==========================================\n",
        "\n",
        "class GlobalStreamBuffer:\n",
        "    \"\"\"\n",
        "    Unified circular buffer holding raw and processed data for both stereo channels.\n",
        "    Allows for historical lookups and snapshot extraction.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, ma_keys):\n",
        "        self.size = size\n",
        "        self.ma_keys = ma_keys\n",
        "        self.data = {}\n",
        "        self.data['time'] = np.zeros(size, dtype=np.float32)\n",
        "\n",
        "        for ch in ['ADC1', 'ADC2']:\n",
        "            self.data[f'{ch}_raw'] = np.zeros(size, dtype=np.float32)\n",
        "            for k in ma_keys:\n",
        "                self.data[f'{ch}_{k}'] = np.zeros(size, dtype=np.float32)\n",
        "\n",
        "        self.ptr = 0\n",
        "        self.total_samples = 0\n",
        "        self.is_full = False\n",
        "\n",
        "    def push(self, time_sec, raw1, ma1, raw2, ma2):\n",
        "        \"\"\"Inserts a new sample set into the circular buffer.\"\"\"\n",
        "        p = self.ptr\n",
        "        self.data['time'][p] = time_sec\n",
        "\n",
        "        self.data['ADC1_raw'][p] = raw1\n",
        "        for k, v in ma1.items():\n",
        "            self.data[f'ADC1_{k}'][p] = v\n",
        "\n",
        "        self.data['ADC2_raw'][p] = raw2\n",
        "        for k, v in ma2.items():\n",
        "            self.data[f'ADC2_{k}'][p] = v\n",
        "\n",
        "        self.ptr = (self.ptr + 1) % self.size\n",
        "        self.total_samples += 1\n",
        "        if self.ptr == 0: self.is_full = True\n",
        "\n",
        "    def get_values_at_offset(self, channel, offset_from_now):\n",
        "        \"\"\"Retrieves data from 'offset_from_now' samples ago.\"\"\"\n",
        "        if offset_from_now > self.total_samples: return None\n",
        "        idx = (self.ptr - offset_from_now + self.size) % self.size\n",
        "\n",
        "        res = {\n",
        "            'time': self.data['time'][idx],\n",
        "            'raw': self.data[f'{channel}_raw'][idx]\n",
        "        }\n",
        "        for k in self.ma_keys:\n",
        "            res[k] = self.data[f'{channel}_{k}'][idx]\n",
        "        return res\n",
        "\n",
        "    def check_cascade_history(self, channel, duration_samples):\n",
        "        \"\"\"\n",
        "        Validates if the cascade condition (sorted MA values) held true\n",
        "        for the specified duration.\n",
        "        \"\"\"\n",
        "        if self.total_samples < duration_samples: return False\n",
        "        for i in range(1, duration_samples + 1):\n",
        "            idx = (self.ptr - i + self.size) % self.size\n",
        "            vals = [self.data[f'{channel}_{k}'][idx] for k in CASCADE_KEYS]\n",
        "            # Check if MAs are strictly ordered (ascending/descending logic)\n",
        "            is_sorted = all(vals[j] >= vals[j+1] for j in range(len(vals)-1))\n",
        "            if not is_sorted: return False\n",
        "        return True\n",
        "\n",
        "    def extract_stereo_snapshot(self, start_time, end_time):\n",
        "        \"\"\"Extracts a slice of buffer data for saving to disk.\"\"\"\n",
        "        mask = (self.data['time'] >= start_time) & (self.data['time'] <= end_time)\n",
        "        indices = np.where(mask)[0]\n",
        "        snippet = []\n",
        "        for i in indices:\n",
        "            row = {\n",
        "                'Abs_Time_Sec': self.data['time'][i],\n",
        "                'ADC1_Raw': self.data['ADC1_raw'][i],\n",
        "                'ADC2_Raw': self.data['ADC2_raw'][i]\n",
        "            }\n",
        "            for k in self.ma_keys:\n",
        "                row[f'ADC1_{k}'] = self.data[f'ADC1_{k}'][i]\n",
        "                row[f'ADC2_{k}'] = self.data[f'ADC2_{k}'][i]\n",
        "            snippet.append(row)\n",
        "        snippet.sort(key=lambda x: x['Abs_Time_Sec'])\n",
        "        return snippet\n",
        "\n",
        "\n",
        "class ChannelProcessor:\n",
        "    \"\"\"\n",
        "    State machine handling peak detection logic for a single channel.\n",
        "    States: SEARCH -> ACTIVE -> (Record & Save) -> SEARCH\n",
        "    \"\"\"\n",
        "    def __init__(self, config, id_gen, global_buffer):\n",
        "        self.cfg = config\n",
        "        self.name = config['name']\n",
        "        self.id_gen = id_gen\n",
        "        self.buffer = global_buffer\n",
        "        self.state = 'SEARCH'\n",
        "        self.current_peak = None\n",
        "        self.pending_peaks = deque()\n",
        "\n",
        "    def process(self, time_sec, raw_val, ma_values, sample_count_global):\n",
        "        \"\"\"Main step function called for every new sample.\"\"\"\n",
        "        self._check_pending_queue(time_sec)\n",
        "        if sample_count_global < WARMUP_SAMPLES: return\n",
        "\n",
        "        slow_vals = [ma_values[k] for k in SLOW_KEYS]\n",
        "        envelope = max(slow_vals)\n",
        "        base_ma = ma_values[BASE_MA_KEY]\n",
        "\n",
        "        if self.state == 'SEARCH':\n",
        "            self._logic_search(time_sec, ma_values, envelope, base_ma)\n",
        "        elif self.state == 'ACTIVE':\n",
        "            self._logic_active(time_sec, raw_val, ma_values, envelope)\n",
        "\n",
        "    def _check_pending_queue(self, current_time):\n",
        "        \"\"\"Checks if any finished peaks are ready to be saved to disk.\"\"\"\n",
        "        while self.pending_peaks:\n",
        "            p = self.pending_peaks[0]\n",
        "            recording_end = p['Start_Time'] + (RECORDING_WINDOW_MS / 1000.0)\n",
        "            if current_time >= recording_end:\n",
        "                self._save_pending_peak(p, recording_end)\n",
        "                self.pending_peaks.popleft()\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    def _save_pending_peak(self, p, recording_end):\n",
        "        snip_start = p['Start_Time'] - (SNIPPET_PRE_MS / 1000.0)\n",
        "        snippet = self.buffer.extract_stereo_snapshot(snip_start, recording_end)\n",
        "        save_peak_data(p, snippet)\n",
        "\n",
        "    def _logic_search(self, time_sec, ma_values, envelope, base_ma):\n",
        "        # Trigger Condition: MA0001 exceeds threshold\n",
        "        trigger_val = ma_values['MA0001']\n",
        "        is_mayor = trigger_val > self.cfg['mayor_thresh']\n",
        "        is_minor = trigger_val > (base_ma + self.cfg['minor_offset'])\n",
        "\n",
        "        if not (is_mayor or is_minor): return\n",
        "\n",
        "        # Persistence Check: Cascade stability\n",
        "        has_cascade = self.buffer.check_cascade_history(self.name, PERSISTENCE_MAYOR_SAMPLES)\n",
        "        if not has_cascade: return\n",
        "\n",
        "        # Peak Confirmation\n",
        "        peak_type = 'Mayor_Natural' if is_mayor else 'Minor'\n",
        "        start_offset = PERSISTENCE_MAYOR_SAMPLES # Backtrack slightly to capture onset\n",
        "        start_data = self.buffer.get_values_at_offset(self.name, start_offset)\n",
        "        self._start_peak(peak_type, start_data)\n",
        "\n",
        "    def _start_peak(self, p_type, start_data):\n",
        "        self.state = 'ACTIVE'\n",
        "        self.current_peak = {\n",
        "            'Peak_Event_ID': next(self.id_gen),\n",
        "            'Channel': self.name,\n",
        "            'Type': p_type,\n",
        "            'Start_Time': start_data['time'],\n",
        "            'Start_Values': {k: start_data[k] for k in MA_KEYS},\n",
        "            'Active_Layers': {k: True for k in MA_KEYS},\n",
        "            'Areas': {k: 0.0 for k in MA_KEYS},\n",
        "            'Flag_Occlusion': 0,\n",
        "            'Max_Amplitude': start_data['raw'],\n",
        "            'End_Time': None\n",
        "        }\n",
        "\n",
        "    def _logic_active(self, time_sec, raw_val, ma_values, envelope):\n",
        "        p = self.current_peak\n",
        "        duration = time_sec - p['Start_Time']\n",
        "\n",
        "        # Update Amplitude\n",
        "        if raw_val > p['Max_Amplitude']:\n",
        "            p['Max_Amplitude'] = raw_val\n",
        "\n",
        "        # Upgrade Minor to Mayor if threshold crossed during event\n",
        "        if p['Type'] == 'Minor' and ma_values['MA0001'] > self.cfg['mayor_thresh']:\n",
        "            p['Type'] = 'Mayor_Natural'\n",
        "\n",
        "        # 1. Occlusion Check (Max Duration Limit)\n",
        "        if duration >= (MAX_PEAK_DURATION_MS / 1000.0):\n",
        "            p['Flag_Occlusion'] = 1\n",
        "            self._close_peak(time_sec)\n",
        "            return\n",
        "\n",
        "        # 2. Layer Energy Integration & Exit Logic\n",
        "        for k in MA_KEYS:\n",
        "            if not p['Active_Layers'][k]: continue\n",
        "\n",
        "            val = ma_values[k]\n",
        "            baseline = min(p['Start_Values'][k], envelope)\n",
        "\n",
        "            if val > baseline:\n",
        "                p['Areas'][k] += (val - baseline) * TIME_DELTA_SEC\n",
        "\n",
        "            # Exit condition for specific layer\n",
        "            is_below_refs = (val < envelope) or (val < p['Start_Values'][k])\n",
        "            is_in_mayor_domain = val > self.cfg['mayor_thresh']\n",
        "            should_exit = is_below_refs and (not is_in_mayor_domain)\n",
        "\n",
        "            if should_exit:\n",
        "                p['Active_Layers'][k] = False\n",
        "\n",
        "        # 3. Global Exit (All layers finished)\n",
        "        if not any(p['Active_Layers'].values()):\n",
        "            self._close_peak(time_sec)\n",
        "\n",
        "    def _close_peak(self, current_time):\n",
        "        p = self.current_peak\n",
        "        p['End_Time'] = current_time\n",
        "        self.pending_peaks.append(p)\n",
        "        self.state = 'SEARCH'\n",
        "        self.current_peak = None\n",
        "\n",
        "# ==========================================\n",
        "# 3. IO & UTILITY FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def id_gen_func():\n",
        "    \"\"\"Generator for unique peak IDs.\"\"\"\n",
        "    i = 0\n",
        "    while True:\n",
        "        i += 1\n",
        "        yield i\n",
        "GLOBAL_ID_GEN = id_gen_func()\n",
        "\n",
        "def init_csvs():\n",
        "    \"\"\"Initializes output CSV files with headers.\"\"\"\n",
        "    with open(OUT_LOG_FILE, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        header = [\n",
        "            'Peak_Event_ID', 'Channel', 'Type', 'Start_Time_Abs', 'End_Time_Abs',\n",
        "            'Duration_ms', 'Max_Amplitude', 'Flag_Occlusion',\n",
        "            'Area_MA0001', 'Area_MA0055'\n",
        "        ]\n",
        "        writer.writerow(header)\n",
        "\n",
        "    with open(OUT_DUMP_FILE, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        header = ['Peak_Event_ID', 'Abs_Time_Sec', 'Rel_Time_ms', 'ADC1_Raw', 'ADC2_Raw']\n",
        "        for k in MA_KEYS: header.append(f'ADC1_{k}')\n",
        "        for k in MA_KEYS: header.append(f'ADC2_{k}')\n",
        "        writer.writerow(header)\n",
        "\n",
        "    with open(OUT_STATS_FILE, 'w') as f:\n",
        "        f.write(\"Batch_ID,System_Time,Processing_Latency_microsecond\\n\")\n",
        "\n",
        "def save_peak_data(p, snippet):\n",
        "    \"\"\"Writes peak metadata and raw snippet data to CSVs.\"\"\"\n",
        "    duration_ms = (p['End_Time'] - p['Start_Time']) * 1000.0\n",
        "\n",
        "    row = [\n",
        "        p['Peak_Event_ID'],\n",
        "        p['Channel'],\n",
        "        p['Type'],\n",
        "        f\"{p['Start_Time']:.4f}\",\n",
        "        f\"{p['End_Time']:.4f}\",\n",
        "        f\"{duration_ms:.1f}\",\n",
        "        f\"{p['Max_Amplitude']:.1f}\",\n",
        "        p['Flag_Occlusion'],\n",
        "        f\"{p['Areas']['MA0001']:.2f}\",\n",
        "        f\"{p['Areas']['MA0055']:.2f}\"\n",
        "    ]\n",
        "\n",
        "    with open(OUT_LOG_FILE, 'a', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(row)\n",
        "\n",
        "    rows_dump = []\n",
        "    for s in snippet:\n",
        "        rel = (s['Abs_Time_Sec'] - p['Start_Time']) * 1000.0\n",
        "        r = [\n",
        "            p['Peak_Event_ID'],\n",
        "            f\"{s['Abs_Time_Sec']:.4f}\",\n",
        "            f\"{rel:.1f}\",\n",
        "            f\"{s['ADC1_Raw']:.1f}\",\n",
        "            f\"{s['ADC2_Raw']:.1f}\"\n",
        "        ]\n",
        "        for k in MA_KEYS: r.append(f\"{s[f'ADC1_{k}']:.1f}\")\n",
        "        for k in MA_KEYS: r.append(f\"{s[f'ADC2_{k}']:.1f}\")\n",
        "        rows_dump.append(r)\n",
        "\n",
        "    with open(OUT_DUMP_FILE, 'a', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(rows_dump)\n",
        "\n",
        "# ==========================================\n",
        "# 4. MAIN EXECUTION ENGINE\n",
        "# ==========================================\n",
        "\n",
        "def run_stream_simulation():\n",
        "    print(\"\\n--- Starting Stream Engine ---\")\n",
        "    init_csvs()\n",
        "\n",
        "    # --- INPUT FILE VALIDATION ---\n",
        "    if not os.path.exists(FILE_PATH):\n",
        "        print(f\"\\n[ERROR] Input file '{INPUT_FILENAME}' not found in the current directory.\")\n",
        "        print(\"Please upload the .csv file to the Colab files area (left sidebar).\")\n",
        "        print(\"If using a different file, update the 'INPUT_FILENAME' variable in the config.\")\n",
        "        return False\n",
        "\n",
        "    print(\"Loading CSV feed (Float32)...\")\n",
        "    try:\n",
        "        df_feed = pd.read_csv(FILE_PATH, dtype=np.float32)\n",
        "        # Normalize column names if necessary\n",
        "        df_feed = df_feed.rename(columns={'adc2': 'adc2', 'adc1': 'adc1'})\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV: {e}\")\n",
        "        return False\n",
        "\n",
        "    # --- PRE-CALCULATE MOVING AVERAGES ---\n",
        "    print(\"Pre-calculating MAs for simulation feed...\")\n",
        "    feed_dict = {}\n",
        "    for ch in ['ADC1', 'ADC2']:\n",
        "        raw_col = 'adc1' if ch == 'ADC1' else 'adc2'\n",
        "        feed_dict[f'{ch}_RAW'] = df_feed[raw_col].values\n",
        "        for k, win in zip(MA_KEYS, MA_WINDOWS_MS):\n",
        "            w_size = int(win / SAMPLE_RATE_MS)\n",
        "            # Rolling mean\n",
        "            feed_dict[f'{ch}_{k}'] = df_feed[raw_col].rolling(w_size).mean().fillna(0).astype(np.float32).values\n",
        "\n",
        "    total_samples = len(df_feed)\n",
        "    print(f\"Feed ready: {total_samples} samples\")\n",
        "\n",
        "    # --- INITIALIZE PROCESSORS ---\n",
        "    global_buffer = GlobalStreamBuffer(BUFFER_LEN, MA_KEYS)\n",
        "    proc1 = ChannelProcessor(CONFIG_ADC1, GLOBAL_ID_GEN, global_buffer)\n",
        "    proc2 = ChannelProcessor(CONFIG_ADC2, GLOBAL_ID_GEN, global_buffer)\n",
        "\n",
        "    print(\"Streaming started...\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    adc1_raw_arr = feed_dict['ADC1_RAW']\n",
        "    adc2_raw_arr = feed_dict['ADC2_RAW']\n",
        "\n",
        "    # --- MAIN LOOP ---\n",
        "    for i in range(total_samples):\n",
        "        loop_start = time.time()\n",
        "        t_sim = i * TIME_DELTA_SEC\n",
        "\n",
        "        # Construct current data packet\n",
        "        row1 = {k: feed_dict[f'ADC1_{k}'][i] for k in MA_KEYS}\n",
        "        row2 = {k: feed_dict[f'ADC2_{k}'][i] for k in MA_KEYS}\n",
        "\n",
        "        # Push to buffer\n",
        "        global_buffer.push(t_sim, adc1_raw_arr[i], row1, adc2_raw_arr[i], row2)\n",
        "\n",
        "        # Process logic\n",
        "        proc1.process(t_sim, adc1_raw_arr[i], row1, i)\n",
        "        proc2.process(t_sim, adc2_raw_arr[i], row2, i)\n",
        "\n",
        "        # Stats logging (Latency check)\n",
        "        if i % 5000 == 0 and i > 0:\n",
        "            loop_end = time.time()\n",
        "            lat_us = (loop_end - loop_start) * 1_000_000\n",
        "            with open(OUT_STATS_FILE, 'a') as f:\n",
        "                f.write(f\"{i},{time.time()},{lat_us:.1f}\\n\")\n",
        "\n",
        "    print(f\"Simulation Done. Real-world execution time: {time.time()-t0:.2f}s\")\n",
        "    return True\n",
        "\n",
        "# ==========================================\n",
        "# 5. POST-PROCESSING & PLOTTING\n",
        "# ==========================================\n",
        "\n",
        "def analyze_and_plot_top_peaks_final():\n",
        "    print(\"\\n--- Starting Post-Processing Analysis ---\")\n",
        "\n",
        "    if not os.path.exists(OUT_LOG_FILE) or not os.path.exists(OUT_DUMP_FILE):\n",
        "        print(\"Output files not found. Simulation might have failed.\")\n",
        "        return\n",
        "\n",
        "    df_log = pd.read_csv(OUT_LOG_FILE)\n",
        "    if df_log.empty:\n",
        "        print(\"Log is empty. No peaks detected.\")\n",
        "        return\n",
        "\n",
        "    df_dump = pd.read_csv(OUT_DUMP_FILE)\n",
        "\n",
        "    # --- STATISTICS SUMMARY ---\n",
        "    print(\"\\n[STATISTICS SUMMARY]\")\n",
        "    total_mayor = len(df_log[df_log['Type'].str.contains('Mayor')])\n",
        "    total_minor = len(df_log[df_log['Type'] == 'Minor'])\n",
        "    count_occlusions = df_log['Flag_Occlusion'].sum()\n",
        "\n",
        "    stats_text = (\n",
        "        f\"Total Peaks: {len(df_log)}\\n\"\n",
        "        f\"  - Mayor: {total_mayor}\\n\"\n",
        "        f\"  - Minor: {total_minor}\\n\"\n",
        "        f\"Peaks Occluded (Cut at {MAX_PEAK_DURATION_MS}ms): {count_occlusions}\\n\"\n",
        "    )\n",
        "    print(stats_text)\n",
        "\n",
        "    with open(os.path.join(OUTPUT_DIR, f'final_stats_{TIMESTAMP_STR}.txt'), 'w') as f:\n",
        "        f.write(stats_text)\n",
        "\n",
        "    # --- PLOTTING LOGIC (Clustered) ---\n",
        "    # Limit to first 50 peaks for safety\n",
        "    candidates_df = df_log.sort_values('Start_Time_Abs').head(50)\n",
        "    print(f\"Generating plots for {len(candidates_df)} peaks...\")\n",
        "\n",
        "    # Cluster peaks that are close in time to reduce figure count\n",
        "    clusters = []\n",
        "    current_cluster = []\n",
        "\n",
        "    for _, peak in candidates_df.iterrows():\n",
        "        if not current_cluster:\n",
        "            current_cluster.append(peak)\n",
        "        else:\n",
        "            prev_peak = current_cluster[-1]\n",
        "            # Group if within 100ms\n",
        "            if peak['Start_Time_Abs'] < prev_peak['Start_Time_Abs'] + 0.100:\n",
        "                current_cluster.append(peak)\n",
        "            else:\n",
        "                clusters.append(current_cluster)\n",
        "                current_cluster = [peak]\n",
        "    if current_cluster: clusters.append(current_cluster)\n",
        "\n",
        "    for cluster in clusters:\n",
        "        anchor_peak = cluster[0]\n",
        "        t_zero = anchor_peak['Start_Time_Abs']\n",
        "\n",
        "        # Determine View Window\n",
        "        view_start = t_zero - (PLOT_PRE_START_MS / 1000.0)\n",
        "        view_end = t_zero + (PLOT_POST_START_MS / 1000.0)\n",
        "\n",
        "        scene_data = df_dump[\n",
        "            (df_dump['Abs_Time_Sec'] >= view_start) &\n",
        "            (df_dump['Abs_Time_Sec'] <= view_end)\n",
        "        ]\n",
        "        if scene_data.empty: continue\n",
        "\n",
        "        fig, axes = plt.subplots(2, 1, figsize=PLOT_FIGSIZE, sharex=True, dpi=PLOT_DPI)\n",
        "\n",
        "        for i, ch in enumerate(['ADC1', 'ADC2']):\n",
        "            ax = axes[i]\n",
        "            # Plot Raw & MA lines\n",
        "            ax.plot(scene_data['Abs_Time_Sec'], scene_data[f'{ch}_Raw'], color='black', lw=1.5, alpha=0.8, label='Raw')\n",
        "            ax.plot(scene_data['Abs_Time_Sec'], scene_data[f'{ch}_MA0001'], color='green', lw=1, label='MA0001')\n",
        "\n",
        "            # Plot Thresholds\n",
        "            cfg = CONFIG_ADC1 if ch == 'ADC1' else CONFIG_ADC2\n",
        "            ax.axhline(cfg['mayor_thresh'], color='purple', ls=':', alpha=0.5, label='Mayor Thr')\n",
        "\n",
        "            # Highlight detected regions\n",
        "            peaks_in_cluster = [p for p in cluster if p['Channel'] == ch]\n",
        "            for p in peaks_in_cluster:\n",
        "                ax.axvline(x=p['Start_Time_Abs'], color='green', alpha=0.4, lw=2)\n",
        "                ax.axvline(x=p['End_Time_Abs'], color='red', alpha=0.4, lw=2)\n",
        "\n",
        "            ax.set_title(f\"{ch} Activity\", fontsize=14)\n",
        "            if i == 0: ax.legend(loc='upper right', fontsize=8)\n",
        "\n",
        "        axes[1].set_xlabel(\"Time (s)\")\n",
        "        is_occ = anchor_peak['Flag_Occlusion'] == 1\n",
        "        title_color = 'red' if is_occ else 'black'\n",
        "        plt.suptitle(f\"Peak Event #{anchor_peak['Peak_Event_ID']} (Starts: {t_zero:.4f}s)\", color=title_color, fontsize=16)\n",
        "\n",
        "        fname = f\"plot_event_{anchor_peak['Peak_Event_ID']}_{TIMESTAMP_STR}.jpg\"\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, fname), bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "\n",
        "    print(\"Plotting finished.\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. ENTRY POINT\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    success = run_stream_simulation()\n",
        "\n",
        "    if success:\n",
        "        analyze_and_plot_top_peaks_final()\n",
        "\n",
        "        # --- AUTO-ZIP OUTPUTS ---\n",
        "        print(\"\\n--- Zipping Results ---\")\n",
        "        shutil.make_archive('processing_results', 'zip', OUTPUT_DIR)\n",
        "        print(f\"Success! Download 'processing_results.zip' from the files tab.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLIsjTreyuAm",
        "outputId": "66baef9d-9c46-4fc3-e3be-c393084eed56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- System Configuration ---\n",
            "Target Input File: resampled_signal_2000.csv\n",
            "Sample Rate: 2000 Hz (Delta: 0.000500 s)\n",
            "\n",
            "--- Starting Stream Engine ---\n",
            "Loading CSV feed (Float32)...\n",
            "Pre-calculating MAs for simulation feed...\n",
            "Feed ready: 2115627 samples\n",
            "Streaming started...\n",
            "Simulation Done. Real-world execution time: 35.12s\n",
            "\n",
            "--- Starting Post-Processing Analysis ---\n",
            "\n",
            "[STATISTICS SUMMARY]\n",
            "Total Peaks: 49\n",
            "  - Mayor: 38\n",
            "  - Minor: 11\n",
            "Peaks Occluded (Cut at 70.0ms): 42\n",
            "\n",
            "Generating plots for 49 peaks...\n",
            "Plotting finished.\n",
            "\n",
            "--- Zipping Results ---\n",
            "Success! Download 'processing_results.zip' from the files tab.\n"
          ]
        }
      ]
    }
  ]
}